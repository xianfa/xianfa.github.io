---
categories: [深度学习]
tags: [tensorflow, 回归分析, 线性回归]
---
环境搭建好以后就要进行下简单的实践，初看了下官网的第一个实例教程，感觉不适合我这种菜鸟入门，请教了搜索大神后，看到了莫烦phton的入门，感觉挺好，就把他的例子作为自己入门的程序
# 例子代码
```
(venv) xianfa@xianfa:~$ cat liner.py 
import tensorflow as tf
import numpy as np
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

#样本数据
x_data = np.random.rand(20).astype(np.float32)
y_data = x_data*3.0 + 5.0

#打印样本数据信息
print(len(x_data))
print('x_data:' + str(x_data))
print('y_data:' + str(y_data))

#初始参数
Weights = tf.Variable(tf.random_uniform([1], - 1.0, 1.0))
biases = tf.Variable(tf.zeros([1]))

#估计方程
y = Weights*x_data + biases

#训练方法使用梯度下降方法 训练目标是使方差最小(差方得均值是不是是不是就是方差 记不大清)
loss = tf.reduce_mean(tf.square(y-y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

#初始化变量
init = tf.global_variables_initializer()

#用Session来执行 前面相当于指定怎么做，这里才是真正的执行
sess = tf.Session()
sess.run(init)

#做20次迭代(这里称为训练)，打印出当前拟合的参数
for step in range(20):
    sess.run(train)
    print(step,sess.run(Weights), sess.run(biases))
```
# 代码分析
我按照自己的想法注释并修改程序，如果有错误，欢迎给我邮件指出。这是一个简单的线性回归问题，就目前所知，ML，DL一般涉及两类问题，及回归和分类。所谓的回归就是给定大量样本，然后我们需要总结出规律，就是这些样本符合怎样的函数分布，然后我们就可以根据得出来的函数，根据给出一些自变量，来给出对他们的预测值。本例子给的是一个一元线性函数及y=ax+b问题，目标就是求出a，b的值。
即使很简单的小程序，如果我们深入的学习也能学到很多的知识。我首先执行下看看给出直观的结果：
```
(venv) xianfa@xianfa:~$ python liner.py 

sample data length:20

x_data:[0.40213642 0.00443163 0.2917779  0.09287278 0.03120894 0.32427305
 0.8610015  0.48237428 0.85973275 0.8990593  0.8525034  0.7159672
 0.20978756 0.4497948  0.17404951 0.08903528 0.48590443 0.46575752
 0.7619489  0.7275428 ]

y_data:[6.2064095 5.0132947 5.875334  5.2786183 5.093627  5.9728193 7.5830045
 6.4471226 7.5791984 7.697178  7.5575104 7.1479015 5.6293626 6.3493843
 5.5221486 5.267106  6.457713  6.3972726 7.2858467 7.1826286]

0 [3.1652653] [6.3938684]
1 [2.476068] [4.924134]
2 [2.6672864] [5.240515]
3 [2.6561894] [5.1527348]
4 [2.6887014] [5.157829]
5 [2.70917] [5.1429043]
6 [2.7303803] [5.1335077]
7 [2.7495728] [5.123771]
8 [2.7675061] [5.1149607]
9 [2.784131] [5.106728]
10 [2.7995727] [5.0990963]
11 [2.8139083] [5.0920076]
12 [2.827219] [5.085427]
13 [2.8395774] [5.0793166]
14 [2.851052] [5.073643]
15 [2.8617058] [5.0683756]
16 [2.8715978] [5.063485]
17 [2.880782] [5.058944]
18 [2.8893092] [5.054728]
19 [2.8972266] [5.0508137]
```
确实离我们给的3和5越来越近，想到了学的一门课叫数值逼近，跟这个差不多吧，可惜没有好好学，有时间再补一补。
首先是
```
import tensorflow as tf
import numpy as np
```
这两句就是导入相应的库，对于我这种python新手说，显然第一个问题就是numpy库可以干什么，提供了那些方法来使用，这些都是需要学习的。

然后是
```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
```
这个是我自己加的，主要是为了不显示某些提示GPU相关的警告，作为学习，cpu就够了，道理是一样的，如果是实际应用一定要直接GPU，我不用GPU是为了尽快学习这些基础知识，GPU方面先暂放。当然了作为程序员，警告最好还是能保留，任何警告都有可能是一个bug的根源。
接着往下
```
#样本数据
x_data = np.random.rand(20).astype(np.float32)
y_data = x_data*3.0 + 5.0
```
我们随机了20个样本数据，X是随机的，y是依赖于x的，这里我们首先就能发现一个问题，实际的样本数据不可能这么漂亮，也就是y值应该有偏差，为了符合实际我们可以 y_data = x_data*3.0 + 5.0 + noise(很小的随机的正负数)，这样原始数据就更贴合实际。
还可以继续延申，比如幂函数，指数函数，对数函数，三角函数。这些还只是一元函数的扩展，我们还可以考虑将x扩展为2维，然后推到任意维度。好想法，但是这些怎么实现呢？变一元函数的形式容易，扩展到二维，抱歉我现在还真不知道。今天我就实现这些想法，再看看具体结果。
下面打印的代码就不用分析了，方便查看用的，再下面
```
#初始参数
Weights = tf.Variable(tf.random_uniform([1], - 1.0, 1.0))
biases = tf.Variable(tf.zeros([1]))

#估计方程
y = Weights*x_data + biases

#训练方法使用梯度下降方法 训练目标是使方差最小(差方得均值是不是是不是就是方差 记不大清)
loss = tf.reduce_mean(tf.square(y-y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)
```
这里是给出我们估计函数的形态，还有初始参数，这里又会冒出好多的问题，我能想到的第一个就是在我们面对具体问题的时候，怎么能够知道我们需要拟合的函数形态？然后假设有两个都可以拟合怎么判别哪个更好？
reduce_mean是什么？张量是什么？张量与数学里的向量，矩阵有何区别？这里我简单写下自己对张量的认识，表示形式上它与矩阵无差别，所不同的应该在于计算方法上。
下面就是optimizer，梯度下降是怎么实现的？除了梯度下降还有那些优化的方法？看了点大牛的视频貌似都在讲这些，对于本例，我首先想到的是在计算的时候会计算出样本的方差么？既然方差是个二次函数，那么它是否在改变weights或biases的时候是加上了一个确定的一次函数呢？会不会出现后一次迭代比前面一次的方差更大的情况？遇到这样如何处理，继续迭代还是回到前面？越学约觉得自己需要学的东西越多。
基本就分析到这里，后面的执行，记住就行了，那就是一个固定的形式。
# 学习总结
我学到回归分析的基本处理过程，抽出了学习方法，我认为比较好的适合于我的学习方法应该是这样，Copy源码，修改调试源码，重新从零写出来源码，改进使之更好。前面两步是学习，后面就是把它变成自己的东西，最后就是创新。学习绝不能简单停留在看过，听过，一定要眼手脑并用，尤其对于程序必须有从零写出来的能力。我是菜鸟，我需要，加油！