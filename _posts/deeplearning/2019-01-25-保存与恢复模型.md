---
categories: [深度学习]
tags: [tensorflow, Keras, 机器学习, 保存模型, 恢复模型]
---
在过拟合和欠拟合的学习过程中，无意中看到有人提到的 机器学习速成教程，谷歌出品，必属经典，赶紧进去简略看了些，确实通俗易懂，刚开始入门学这个会好些，后面继续学习的时候会顺道把这个过一遍。有点不爽的是，打开谷歌的开发者的主页，无法从菜单导航到速成教程，不过从搜索框输入机器学习四个字就可以看到了，感觉google这里做的稍微有些缺憾，也有可能是我自己没找对。
# 问题描述
这次学的是模型的保存与恢复，在程序员的世界里就是序列化和反序列化。这里需要保存的就是模型，以及训练的权重。对于模型无非就是模型的参数，每层的类型，数据输入输出形式，激励函数等等。训练的权重可以理解为回归分析中的参数，每次训练也就是调整这些参数，使其拟合的误差更小些。权重的保存可以分为两个，训练过程中以及训练完成后结果的保存，训练过程中的保存是为了后面可以接着当前训练进度继续训练，训练结果的保存是为了以后可以直接使用现有的成果，用途上的差别是一个是给自己用的，另一个是给别人用的，其本质上相同。  
# 保存恢复源码
```
(venv) xianfa@xianfa:~/machinelearning/study$ cat saveloadmodel.py 
import tensorflow as tf
from tensorflow import keras
import os

#print cmd execute info to console
def execcmd(cmdstr):
    strlines = os.popen(cmdstr).readlines()
    print('\ncmdstr:' + cmdstr + '\n')
    for line in strlines:
        print(line, end='')
    print('\n')


# Returns a short sequential model
def create_model():
    model = tf.keras.models.Sequential([
        keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(784,)),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(10, activation=tf.nn.softmax)
        ])

    model.compile(optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    metrics=['accuracy'])
    return model

if __name__ == '__main__':
    #prehandle data
    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
    
    train_labels = train_labels[:1000]
    test_labels = test_labels[:1000]

    train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
    test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0

    # Create a basic model instance show summary info
    model = create_model()
    model.summary()

    #save training process use ModelCheckpoint
    print('\n**********Test save training process use ModelCheckpoint')
    checkpoint_path = "training_1/cp.ckpt"
    checkpoint_dir = os.path.dirname(checkpoint_path)

    # Create checkpoint callback
    cp_callback = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path,
        save_weights_only=True,
        verbose=1)

    model = create_model()
    model.fit(
        train_images,
        train_labels,
        epochs = 10,
        validation_data = (test_images,test_labels),
        callbacks = [cp_callback])  # pass callback to training

    execcmd('ls -lh ' + checkpoint_dir)

    model = create_model()
    loss, acc = model.evaluate(test_images, test_labels)
    print("Untrained model, accuracy: {:5.2f}%".format(100*acc))

    model.load_weights(checkpoint_path)
    loss,acc = model.evaluate(test_images, test_labels)
    print("Restored model, accuracy: {:5.2f}%".format(100*acc))

    #save training process use ModelCheckpoint and set the period option
    print('\n**********Test save training process use ModelCheckpoint and set the period option')
    # include the epoch in the file name. (uses `str.format`)
    checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
    checkpoint_dir = os.path.dirname(checkpoint_path)

    cp_callback = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path, verbose=1, save_weights_only=True,
        # Save weights, every 5-epochs.
        period=5)

    model = create_model()
    model.fit(
        train_images,
        train_labels,
        epochs = 50,
        callbacks = [cp_callback],
        validation_data = (test_images,test_labels),
        verbose=0)
    execcmd('ls -lh ' + checkpoint_dir)

    #test load latest checkpoint
    print('\n**********Test load latest checkpoint')
    model = create_model()
    latest = tf.train.latest_checkpoint(checkpoint_dir)
    model.load_weights(latest)
    loss, acc = model.evaluate(test_images, test_labels)
    print("Restored model, accuracy: {:5.2f}%".format(100*acc))

    # Save the weights
    model.save_weights('./checkpoints/my_checkpoint')

    # Restore the weights
    model = create_model()
    model.load_weights('./checkpoints/my_checkpoint')

    loss,acc = model.evaluate(test_images, test_labels)
    print("Restored model, accuracy: {:5.2f}%".format(100*acc))

    #save entire model test
    print('\n**********Test save entire model')
    model = create_model()
    model.fit(train_images, train_labels, epochs=5)

    # Save entire model to a HDF5 file
    model.save('my_model.h5')

    # Recreate the exact same model, including weights and optimizer.
    new_model = keras.models.load_model('my_model.h5')
    new_model.summary()

    loss, acc = new_model.evaluate(test_images, test_labels)
    print("Restored model, accuracy: {:5.2f}%".format(100*acc))
```
# 源码分析
执行结果
```
(venv) xianfa@xianfa:~/machinelearning/study$ python saveloadmodel.py 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 512)               401920    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                5130      
=================================================================
Total params: 407,050
Trainable params: 407,050
Non-trainable params: 0
_________________________________________________________________

**********Test save training process use ModelCheckpoint
Train on 1000 samples, validate on 1000 samples
Epoch 1/10
2019-01-24 06:54:49.724196: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
 992/1000 [============================>.] - ETA: 0s - loss: 1.1468 - acc: 0.6573 
Epoch 00001: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 1s 738us/step - loss: 1.1392 - acc: 0.6600 - val_loss: 0.7391 - val_acc: 0.7560
Epoch 2/10
 960/1000 [===========================>..] - ETA: 0s - loss: 0.4320 - acc: 0.8729
Epoch 00002: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 298us/step - loss: 0.4268 - acc: 0.8730 - val_loss: 0.5334 - val_acc: 0.8330
Epoch 3/10
 800/1000 [=======================>......] - ETA: 0s - loss: 0.2902 - acc: 0.9275
Epoch 00003: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 277us/step - loss: 0.2894 - acc: 0.9290 - val_loss: 0.4847 - val_acc: 0.8420
Epoch 4/10
 832/1000 [=======================>......] - ETA: 0s - loss: 0.2137 - acc: 0.9459
Epoch 00004: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 343us/step - loss: 0.2061 - acc: 0.9500 - val_loss: 0.4550 - val_acc: 0.8560
Epoch 5/10
 896/1000 [=========================>....] - ETA: 0s - loss: 0.1663 - acc: 0.9632
Epoch 00005: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 314us/step - loss: 0.1684 - acc: 0.9620 - val_loss: 0.4442 - val_acc: 0.8600
Epoch 6/10
 992/1000 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9798
Epoch 00006: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 288us/step - loss: 0.1213 - acc: 0.9800 - val_loss: 0.4088 - val_acc: 0.8660
Epoch 7/10
 768/1000 [======================>.......] - ETA: 0s - loss: 0.0871 - acc: 0.9857
Epoch 00007: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 280us/step - loss: 0.0883 - acc: 0.9850 - val_loss: 0.4124 - val_acc: 0.8670
Epoch 8/10
 992/1000 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9899
Epoch 00008: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 356us/step - loss: 0.0689 - acc: 0.9900 - val_loss: 0.4294 - val_acc: 0.8630
Epoch 9/10
 928/1000 [==========================>...] - ETA: 0s - loss: 0.0529 - acc: 0.9968
Epoch 00009: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 311us/step - loss: 0.0556 - acc: 0.9960 - val_loss: 0.4243 - val_acc: 0.8670
Epoch 10/10
 928/1000 [==========================>...] - ETA: 0s - loss: 0.0442 - acc: 0.9968
Epoch 00010: saving model to training_1/cp.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77deb51390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 323us/step - loss: 0.0438 - acc: 0.9970 - val_loss: 0.4178 - val_acc: 0.8640

cmdstr:ls -lh training_1

total 1.6M
-rw-rw-r-- 1 xianfa xianfa   71 Jan 24 06:54 checkpoint
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:54 cp.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:54 cp.ckpt.index


1000/1000 [==============================] - 0s 218us/step
Untrained model, accuracy:  9.90%
1000/1000 [==============================] - 0s 41us/step
Restored model, accuracy: 86.40%

**********Test save training process use ModelCheckpoint and set the period option

Epoch 00005: saving model to training_2/cp-0005.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00010: saving model to training_2/cp-0010.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00015: saving model to training_2/cp-0015.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00020: saving model to training_2/cp-0020.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00025: saving model to training_2/cp-0025.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00030: saving model to training_2/cp-0030.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00035: saving model to training_2/cp-0035.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00040: saving model to training_2/cp-0040.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00045: saving model to training_2/cp-0045.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

Epoch 00050: saving model to training_2/cp-0050.ckpt
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77dc759780>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.

cmdstr:ls -lh training_2

total 16M
-rw-rw-r-- 1 xianfa xianfa   81 Jan 24 06:55 checkpoint
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:54 cp-0005.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:54 cp-0005.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:54 cp-0010.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:54 cp-0010.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:54 cp-0015.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:54 cp-0015.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:55 cp-0020.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:55 cp-0020.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:55 cp-0025.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:55 cp-0025.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:55 cp-0030.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:55 cp-0030.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:55 cp-0035.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:55 cp-0035.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:55 cp-0040.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:55 cp-0040.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:55 cp-0045.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:55 cp-0045.ckpt.index
-rw-rw-r-- 1 xianfa xianfa 1.6M Jan 24 06:55 cp-0050.ckpt.data-00000-of-00001
-rw-rw-r-- 1 xianfa xianfa  647 Jan 24 06:55 cp-0050.ckpt.index



**********Test load latest checkpoint
1000/1000 [==============================] - 0s 152us/step
Restored model, accuracy: 88.00%
WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f77daea1128>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.

Consider using a TensorFlow optimizer from `tf.train`.
1000/1000 [==============================] - 0s 184us/step
Restored model, accuracy: 88.00%

**********Test save entire model
Epoch 1/5
1000/1000 [==============================] - 1s 702us/step - loss: 1.1919 - acc: 0.6490
Epoch 2/5
1000/1000 [==============================] - 0s 234us/step - loss: 0.4248 - acc: 0.8840
Epoch 3/5
1000/1000 [==============================] - 0s 223us/step - loss: 0.2775 - acc: 0.9330
Epoch 4/5
1000/1000 [==============================] - 0s 233us/step - loss: 0.2089 - acc: 0.9550
Epoch 5/5
1000/1000 [==============================] - 0s 220us/step - loss: 0.1622 - acc: 0.9670
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_12 (Dense)             (None, 512)               401920    
_________________________________________________________________
dropout_6 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_13 (Dense)             (None, 10)                5130      
=================================================================
Total params: 407,050
Trainable params: 407,050
Non-trainable params: 0
_________________________________________________________________
1000/1000 [==============================] - 0s 211us/step
Restored model, accuracy: 86.10%
```
执行完毕。以上为控制台输出信息，还有保存的文件。主要的过程我在输出信息中加了10个*号，方便查看。简略说下处理的过程，首先是预处理数据，打印出模型的基本信息，这个是为了和最后的保存模型，然后读出模型方便对比的。接着训练一个模型并使用modelcheckpoint在每次训练完成后，覆盖保存到指一指定文件，然后通过创建空模型和读取模型，通过验证acc说明保存读取成功。后面的例子在前面的基础上，通过设定modelcheckpoint中的period选项为5，使其每训练5次调用回调，并将次数作为参数，保存相应的训练数据，后面调用latest_checkpoint获取到最后保存的数据，采用和前面一样的方法进行验证保存读取有效，这里应该可以读取任意的一个保存的数据，我没有验证。最后就是保存了一个模型和训练的权重数据，后面验证权重的方法同前面一样，模型的话直接打印出了模型的summary，和前面打印模型的summary经过对比后，可以确认一致。
# 知识问题
例程中有用到ls显示一些信息的，直接拷贝进代码里面报错，在C++里可以system(cmdstr)这样用，估计python也差不多肯定也有这样用的，搜索后发现了os模块里面有os.system,试了下os.system(cmdstr),没看到输出信息，接着又查发现了别人总结的python调用shell命令的方法，这里我仅使用os模块的，command模块的我没试。调用系统命令的目的是为了查看文件系统，不是仅获得执行结果的码值，主要是为了获取控制台打印的信息，查阅试验后发现os.popen(cmdstr).readlines可以满足要求。  
在查这个的时候还有一点发现，原先以为脚本语言都是直接按顺序运行的，如果其中有错误，就会停止在出错的地方。所以测试前面的os方法时，都是在文件开头直接打印，运行没打印出来，总是给我报文件后面的错误，把后面错误的代码注释后终于出现了预想的结果。从这里说明，python首先会进行文件的语法检查，有错就报，而不是我原先以为的直接执行，遇到错误的地方停止，然后再报错。  
关于字符串的格式化python有两种实现方式，及%方式和format方式。该程序中本人使用+字符串的方式来实现，我的学习方式是实际问题需要解决然后学相应的东西，不贪求大而全，其实也就是敏捷开发里面的TDD方式。  
程序输出有很多警告，大意是说优化函数使用了keras的，其状态不会被保存。不过手册最后给出提示，使用此类优化器时，您需要在加载模型后对其进行重新编译，使优化器的状态变松散，这点需要注意。  
在程序开发中，各种配置文件，程序生成的文件，对于理解程序至关重要，很多时候把这些文件搞清楚，程序基本也就捋顺了。所以我就粗略的看了下这里生成几个文件，model那个文件就不说了，都是二进制，不看代码一点也没法理解。至于进度一般是两个文件，一个index一个data，index大概可以看到attribut，value，应该就是属性和什么值在data文件的中的索引位置，由于这些文件并没有采用可读的json或者字符格式，所以也只能大概的看一看。  
关于知识的学习，学完这个tensorflow高阶api的简单入门就完成了，后面提到了eagar，学习真的是个巨大的工程，一个知识点的学习，能牵连到一整套知识学习，进而能包揽人类整个的知识海洋，艰巨，恐怖，学习之路漫漫，十辈子也学不完。
最后说明一点，在开始写了觉得谷歌做的有些缺憾，这次我再翻阅到手册的开始部分，发现里面清楚的写着“要了解机器学习的基础知识和概念，建议您学习机器学习速成课程。我们在后续学习计划中列出了其他 TensorFlow 和机器学习资源。”，原来速成教程人家在开始就提示并且还是直接链接，只是自己粗心没发现而已，真心觉得google是目前最好的公司，没有之一，为自己的冒失评论检讨。
# 学习总结
要善于发现自己的弱点，坦诚面对自己的错误。本次学习了模型权重的保存和读取，以及整个模型的保存和读取，可以直接使用大神的模型和训练好的参数来进行预测处理了，想想都觉得是很振奋的事情，期待。后面该怎么继续学习呢？找个实际的项目实现一下练练手？还是继续跟着教程一步一步的学习？我更倾向于前者，实现一个什么呢？打开照相机，然后把自己的脸框出来，这个怎么样？聊天机器人？还有前些时候写了个斗地主游戏，能不能加些AI处理？先记下，还是继续学习，实现这些可不是容易，每个都需要再学不少东东，唯有斗地主看起来还简单些，但是我都不知道将数据处理成什么样子。就这样，带着这个疑问继续学习，有想法了立马实现之。